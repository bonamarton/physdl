{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/patbaa/physdl/blob/master/notebooks/11/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM on IMDB Movie review dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Embedding, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset. Vocabulary size is restricted to 10.000. So only the 10.000 most frequent word will appear in out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_words = 10000 # vocab size\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(index_from=3, seed=42, num_words=N_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually this is not a vector of one-hot encoded vectors. Contains the same information, but requires way less memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11, 4079, 11, 4, 1986, 745, 3304, 299, 1206, 590, 3029, 1042, 37, 47, 27, 1269, 2, 7637, 19, 6, 3586, 15, 1367, 3196, 17, 1002, 723, 1768, 2887, 757, 46, 4, 232, 1131, 39, 107, 3589, 11, 4, 4539, 198, 24, 4, 1834, 133, 4, 107, 7, 98, 413, 8911, 5835, 11, 35, 781, 8, 169, 4, 2179, 5, 259, 334, 3773, 8, 4, 3497, 10, 10, 17, 16, 3381, 46, 34, 101, 612, 7, 84, 18, 49, 282, 167, 2, 7173, 122, 24, 1414, 8, 177, 4, 392, 531, 19, 259, 15, 934, 40, 507, 39, 2, 260, 77, 8, 162, 5097, 121, 4, 65, 304, 273, 13, 70, 1276, 2, 8, 15, 745, 3304, 5, 27, 322, 2197, 2, 2, 70, 30, 2, 88, 17, 6, 3029, 1042, 29, 100, 30, 4943, 50, 21, 18, 148, 15, 26, 5980, 12, 152, 157, 10, 10, 21, 19, 3196, 46, 50, 5, 4, 1636, 112, 828, 6, 1003, 4, 162, 5097, 2, 517, 6, 2, 7, 4, 9527, 5593, 4, 351, 232, 385, 125, 6, 1693, 39, 2383, 5, 29, 69, 5593, 5670, 6, 162, 5097, 1567, 232, 256, 34, 718, 5612, 2980, 8, 6, 226, 762, 7, 2, 7830, 5, 517, 2, 6, 3242, 7, 4, 351, 232, 37, 9, 1861, 8, 123, 3196, 2, 5612, 188, 5165, 857, 11, 4, 86, 22, 121, 29, 1990, 1495, 10, 10, 1276, 61, 514, 11, 14, 22, 9, 1456, 9533, 14, 575, 208, 159, 9533, 16, 2, 5, 187, 15, 58, 29, 93, 6, 2, 7, 395, 62, 30, 1211, 493, 37, 26, 66, 2, 29, 299, 4, 172, 243, 7, 217, 11, 4, 2, 7106, 22, 4, 2, 1038, 13, 70, 243, 7, 3468, 19, 9533, 11, 15, 236, 1313, 136, 121, 29, 5, 5612, 26, 112, 4382, 180, 34, 3304, 1768, 5, 320, 4, 162, 5097, 568, 319, 4, 3324, 5235, 1456, 269, 8, 401, 56, 19, 5612, 16, 142, 334, 88, 146, 243, 7, 11, 2, 2756, 150, 11, 4, 2, 2550, 10, 10, 7173, 828, 4, 206, 170, 33, 6, 52, 4968, 225, 55, 117, 180, 58, 11, 14, 22, 48, 50, 16, 101, 329, 12, 62, 30, 35, 6637, 1532, 22, 4079, 11, 4, 1986, 1199, 35, 735, 18, 118, 204, 881, 15, 291, 10, 10, 7173, 82, 93, 52, 361, 7, 4, 162, 5097, 2, 5, 4, 785, 6542, 49, 7, 4, 172, 2572, 7, 665, 26, 303, 343, 11, 23, 4, 2, 11, 192, 4079, 11, 4, 1986, 9, 44, 84, 24, 2, 54, 36, 66, 144, 11, 68, 205, 118, 602, 55, 729, 174, 8, 23, 4, 2, 10, 10, 4079, 11, 4, 1986, 127, 316, 2606, 37, 16, 3445, 19, 12, 150, 138, 426, 2, 7173, 79, 49, 542, 162, 5097, 4413, 84, 11, 4, 392, 555]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we saw above the dataset is just a bunch of indices. To convert back to the original text we need check the code\n",
    "```python\n",
    "imdb.load_data??\n",
    "```\n",
    "it says\n",
    "```\n",
    "by convention, use 2 as OOV word\n",
    "reserve 'index_from' (=3 by default) characters:\n",
    "0 (padding), 1 (start), 2 (OOV)\n",
    "```\n",
    "\n",
    "So when we need to shift the mapping based on the index_from parameter we set above.   \n",
    "We can load imdb.get_word_index() which is a mapping from the words to the indices that need to be inverted and shifted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2idx = imdb.get_word_index()\n",
    "idx2w = {(w2idx[w]+3):w for w in w2idx.keys()}\n",
    "idx2w[0] = '<PAD>'\n",
    "idx2w[1] = '<START>'\n",
    "idx2w[2] = '<OOV>' # out of vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> in panic in the streets richard widmark plays u s navy doctor who has his week <OOV> interrupted with a corpse that contains plague as cop paul douglas properly points out the guy died from two bullets in the chest that's not the issue here the two of them become unwilling partners in an effort to find the killers and anyone else exposed to the disease br br as was pointed out by any number of people for some reason director <OOV> kazan did not bother to cast the small parts with anyone that sounds like they're from <OOV> having been to new orleans where the story takes place i can personally <OOV> to that richard widmark and his wife barbara <OOV> <OOV> can be <OOV> because as a navy doctor he could be assigned there but for those that are natives it doesn't work br br but with plague out there and the news being kept a secret the new orleans <OOV> starts a <OOV> of the city's underworld the dead guy came off a ship from europe and he had underworld connections a new orleans wise guy played by jack palance jumps to a whole bunch of <OOV> conclusions and starts <OOV> a cousin of the dead guy who is starting to show plague <OOV> palance got rave reviews in the first film where he received notice br br personally my favorite in this film is zero mostel this happened right before mostel was <OOV> and around that time he made a <OOV> of playing would be tough guys who are really <OOV> he plays the same kind of role in the <OOV> bogart film the <OOV> sadly i can kind of identify with mostel in that last chase scene where he and palance are being chased down by widmark douglas and half the new orleans police seeing the weight challenged zero trying to keep up with palance was something else because i'm kind of in <OOV> league now in the <OOV> department br br kazan kept the action going at a good clip there's very little down time in this film if there was any less it would be an indiana jones film panic in the streets won an oscar for best original screenplay that year br br kazan also made good use of the new orleans <OOV> and the french quarter some of the same kinds of shots are later used in on the <OOV> in fact panic in the streets is about people not <OOV> when they really should in their own best interest very similar again to on the <OOV> br br panic in the streets does everyone proud who was associated with it now why couldn't <OOV> kazan get some decent new orleans sounding people in the small roles \n",
      "\n",
      "<START> if you ask me the first one was really better one look at sarah m g she is real mean cruel girl look at amy adams she is just little fool hanging around she is nothing people don't adore her second sebastian was cute and hot in first movie now he is baby face story is not that good and i do not understand why didn't they make this one first it is the beginning <OOV> actors nothing with story this is not cruel this is playing first one has better actors better story and its mean i think that the music is better in cruel intentions 1 and the music is better in cruel intentions 3 it is not the worst movie i saw but in <OOV> with first one its one big big big nothing \n",
      "\n",
      "<START> i am a big fan a <OOV> tale theatre and i've seen them all and this is one of the best it's funny romantic and a classic i recommend this for all ages it's great for little kids because it's well cinderella and great for adults and teen because it's funny and not over the top i watched it when i was little and i still watch it now it has great lines that my family and i quote all the time the acting is great and it never gets old if you like fairy tales and romances you will love this i've watched many a cinderella movie in my time and this is the best of them all sorry disney i highly recommend this movie and all the <OOV> tale theatre shows they all appeal to all ages and are all unique and very entertaining\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([idx2w[i] for i in x_train[0]]), '\\n')\n",
    "print(' '.join([idx2w[i] for i in x_train[1]]), '\\n')\n",
    "print(' '.join([idx2w[i] for i in x_train[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "We could use some pre-trained embedding matrix in the first layer, but as for now it is just randomly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(N_words, 200))\n",
    "model.add(LSTM(200, dropout=0.5, recurrent_dropout=0.3, return_sequences=True)) \n",
    "# note dropout vs recurrent dropout https://arxiv.org/pdf/1512.05287.pdf\n",
    "model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.3, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 200)         2000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 200)         320800    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,441,301\n",
      "Trainable params: 2,441,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 83% of the parameters are in the embedding layer!\n",
    "\n",
    "### What is the usual size of the descriptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467\n",
      "138\n",
      "147\n",
      "168\n",
      "144\n",
      "248\n",
      "125\n",
      "204\n",
      "138\n",
      "182\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): print(len(x_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_x_train = sequence.pad_sequences(x_train, maxlen=200) \n",
    "# if want to work with mini-batches we need same size input otherwise we need to train one-by one (also possible)\n",
    "padded_x_test  = sequence.pad_sequences(x_test, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 90s 4ms/step - loss: 0.4883 - acc: 0.7582 - val_loss: 0.3811 - val_acc: 0.8418\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 88s 4ms/step - loss: 0.3508 - acc: 0.8568 - val_loss: 0.3801 - val_acc: 0.8364\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 89s 4ms/step - loss: 0.3208 - acc: 0.8742 - val_loss: 0.3639 - val_acc: 0.8509\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 89s 4ms/step - loss: 0.2631 - acc: 0.8995 - val_loss: 0.3595 - val_acc: 0.8588\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.2346 - acc: 0.9123 - val_loss: 0.3556 - val_acc: 0.8581\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_x_train, y_train, batch_size=128, epochs=5, validation_data=(padded_x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This result is far from state-of-the-art. It is just an example.\n",
    "\n",
    "You can go up to over 94% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train sample-by-sample with varying sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467\n",
      "138\n",
      "147\n",
      "168\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "for seq, label in zip(x_train[:5], y_train[:5]):\n",
    "    print(len(seq))\n",
    "    model.train_on_batch(np.array(seq)[None], label[None])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
