{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regluarization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patbaa/physdl/blob/master/notebooks/02/regluarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "u0vmAf2Qw17J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model capacity, regularization\n",
        "---\n",
        "\n",
        "\n",
        "Ideally you want to select a model which has just pefect capacity. It's not too complex to overfit, and it's not too simple to underfit. \n",
        "\n",
        "When you know the phenomenon, it is possible to pick a suitable model by hand. But if you don't know anything about the data, how can you achieve it?\n",
        "\n",
        "One solution is to select a model with high capacity, but force it to be simple when possible. This is called regularization.\n",
        "\n",
        "A simpler model generally means that its more smooth, it has fewer non-zero parameters. You can achieve it by simply adding a term to your loss function which penalizes large (non-zero) parameters. \n",
        "\n",
        "$$ Loss = Loss_{error} + Loss_{penalty}$$\n",
        "\n",
        "\n",
        "Popular errors are the L1, and L2 norm of the parameters.\n",
        "\n",
        "$$ Loss = Loss_{error} + \\lambda_1\\sum{|w_i|} + \\lambda_2\\sum{w_i^2} $$\n",
        "\n",
        "\n",
        "L1 is used: it is called Lasso regression. This penalty forces the non-important parameters to be exactly zero. The model will be sparse.\n",
        "\n",
        "L2 is used: it is called Rigde regression (weigth decay). This penalty forces the non-important parameters to be small but not exactly zero. The model will not be sparse.\n",
        "\n",
        "L1 and L2: Elastic net. Also sparse model.\n",
        "\n",
        "----"
      ]
    },
    {
      "metadata": {
        "id": "LubWfdbTw17N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook was created by Dezso Ribli, if you have any remarks or questions, please contact me.\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "G9jivjUnw17Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%pylab inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "abvQPtuOxGFj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "figsize(7,5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oAb7Jaoqw17g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The polynom fitting example\n",
        "\n",
        "* Create noisy data from a second order polynomial relationship.\n",
        "* Try to fit it with a linear model, it will underfit.\n",
        "* Try to fit it with a 10th order polynom it will overfit."
      ]
    },
    {
      "metadata": {
        "id": "86BlUeLJw17i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N = 20  # number of points\n",
        "M = 10  # polynom degree\n",
        "x = linspace(0,1,N) + (random.rand(N)-0.5) *0.1  # x with error\n",
        "y =  (x-0.42)**2 + 4.2 + (random.rand(N)-0.5) *0.1  # y with error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DJ7iI-dMw17n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Just plot it."
      ]
    },
    {
      "metadata": {
        "id": "qr7bk7nhw17p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# smooth x for plotitng lines\n",
        "smoothx = linspace(min(x)-0.01,max(x)+0.01,100)  \n",
        "\n",
        "plot(x,y,'o',label='data')\n",
        "plot(smoothx,(smoothx-0.42)**2 + 4.2, '--',label='true')\n",
        "_=legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s72bkMBww17v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import polinomial features, and linear regression"
      ]
    },
    {
      "metadata": {
        "id": "RYQHcpXpw17x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nR6DDXzVw170",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fit it"
      ]
    },
    {
      "metadata": {
        "id": "EuOC90tUw171",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fit a linear\n",
        "model = LinearRegression()  # init model\n",
        "model.fit(x.reshape(-1,1),y)  # fit it\n",
        "yuf = model.predict(x.reshape(-1,1))\n",
        "\n",
        "# create polinom features\n",
        "xp = PolynomialFeatures(M).fit_transform(x.reshape(-1,1))\n",
        "model = LinearRegression()  # init model\n",
        "model.fit(xp,y)  # fit it\n",
        "\n",
        "# creat smooth y for plotting\n",
        "smoothxp = PolynomialFeatures(M).fit_transform(smoothx.reshape(-1,1))\n",
        "yp = model.predict(smoothxp)  # predict on smooth grid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pFsJHgudw174",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Plot it"
      ]
    },
    {
      "metadata": {
        "id": "x33wYHfJw175",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot(x,y,'o',label='data')\n",
        "plot(x,yuf,'-',label='underfit')\n",
        "plot(smoothx,yp,'-',label='overfit')\n",
        "\n",
        "plot(smoothx,(smoothx-0.42)**2 + 4.2, '--',label='true')\n",
        "_=legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Ac2hp7iw179",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Fit with regularization"
      ]
    },
    {
      "metadata": {
        "id": "l3d5Kt4Hw17-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge  # L2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BCpbakbLw18B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Ridge(alpha =0.001)\n",
        "model.fit(xp,y)\n",
        "smoothxp = PolynomialFeatures(M).fit_transform(smoothx.reshape(-1,1))\n",
        "yp = model.predict(smoothxp)\n",
        "\n",
        "\n",
        "plot(x,y,'o',label='data')\n",
        "plot(smoothx,yp,'-',label='fit')\n",
        "plot(smoothx,(smoothx-0.42)**2 + 4.2, '--',label='true')\n",
        "_=legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dJonQ2jCw18F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Message\n",
        "\n",
        "- Overfitting can be tamed with regularization. Don't be afraid of very complicated models."
      ]
    },
    {
      "metadata": {
        "id": "sja7p97Uw18F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# More: General regularization \n",
        "\n",
        "Note that the penalty causes the original model to be **less** accurate on the training data, but **more** accurate on unseen data. Generally you can call any method a 'regularization' if it does not reduce training error, but reduces test error, therefore improves model generalization.\n",
        "\n",
        "Simple penalties are not the only way to regularize a model. For example neural networks employ a vast army of different regularizations: Dropouts, Batch normalization, early stopping and more. \n",
        "These tricks allow us to train models with hundreds of million of paramaters, and still not overfit (too badly)."
      ]
    }
  ]
}